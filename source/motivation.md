# Why Benchmarking? 

A good benchmarking strategic initiative for artificial intelligence (AI)
involves a systematic and comprehensive approach to discover, map, evaluate, and
compare AI models, algorithms, and systems against what ought to  emerge as
industry standard, taking into account what is already emerging in industry and
research contexts. The purpose of this initiative is to gain insights into the
performance, efficiency, and effectiveness of AI technologies and identify areas
for improvement. 

Conducting benchmarking activities is of paramount relevance for a variety of
organisations to gain valuable insights, drive improvements, and make informed
decisions to enhance their AI systems' performance and competitiveness. The main
target groups that ought to  be interested in this activity are AI practitioners
and researchers that want to understand how their AI assets will behave in a
pool of different hardware (HW) resources.

As AI is becoming more pervasive in a wide variety of domains, it becomes more
and more important to understand the behaviour of AI tools and algorithms on a
broad spectrum of different computing resources. This increasing concern
motivates the benchmarking AI applications and assets, as it is extremely
important to characterise how AI algorithms are running across different
hardware platforms and under different configurations â€“ thus understanding the
behaviour of these algorithms. The effort of benchmarking is made even more
complicated by the fact that AI assets more than likely will change the way they
run and  behave, according to a large number of different (hyper)parameters
governing their behaviour, in addition to the vastly different performance
observed on heterogeneous hardware (HW) devices.

Furthermore, challenges of benchmarking AI will be further compounded by the
fact that, nowadays, many AI applications can be run in and across a wide
spectrum  of hardware (HW) resources, ranging from Cloud and high performance
computing (HPC) resources to embedded and low-power systems. This kind of
diversification of terminal devices and services is happening  because  of a
common trend these days: a steady growth of capacity and performance in embedded
devices, whose growth, in turn, allows these devices  to perform ever more
sophisticated tasks, including new AI applications. Since the advent of AI for
computer vision and speech recognition tasks, training of AI models has been
performed using ML algorithms, with a large number of open-source datasets
created by researchers.
